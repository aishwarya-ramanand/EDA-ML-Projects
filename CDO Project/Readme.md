# ğŸ“Š Machine Learning Project â€“ Predicting Custodial Suicide  
**By Aishwarya Maiya**

Badges:  
`ML` `Data Cleaning` `Feature Engineering` `Model Evaluation` `Classification`

---

## ğŸ§  Overview

This repository showcases a **Machine Learning Classification Project** built using the *Crime in India* dataset from Kaggle.  
The goal is to **predict whether a custodial suicide occurred** based on crime indicators, demographics, and police data.

The project includes **EDA, preprocessing, model training, evaluation, and model export**, implemented using Python and Scikit-Learn.

---

## ğŸ—‚ï¸ Projects

### 1ï¸âƒ£ ğŸ” **Data Loading & Exploration**

- Loaded 18+ CSV files from the Crime in India dataset.
- Selected the **Custodial Deaths & Suicides (CDO)** dataset.
- Performed:
  - `.info()`, `.describe()`
  - Missing value analysis
  - Data type inspection
  - Distribution checks for `Suicide_Occurred`

---

### 2ï¸âƒ£ ğŸ§¹ **Data Preprocessing**

Preprocessing Pipeline:

#### âœ”ï¸ Numerical Features
- Median imputation  
- StandardScaler  

#### âœ”ï¸ Categorical Features
- Most frequent imputation  
- One-hot encoding  

Combined using:
```python
ColumnTransformer(
    transformers=[
        ("num", num_transformer, numerical_features),
        ("cat", cat_transformer, categorical_features)
    ]
)

### 3ï¸âƒ£ Train-Test Split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=42, stratify=y
)

### 4ï¸âƒ£ Modeling

#### Logistic Regression (Baseline)

logpipe = Pipeline(
    steps=[
        ("pre", preprocessor),
        ("logreg", LogisticRegression(class_weight="balanced", random_state=42))
    ]

#### Random Forest (Final Model)

rfpipe = Pipeline(
    steps=[
        ("pre", preprocessor),
        ("rf", RandomForestClassifier(random_state=42))
    ]
)
best_model = rfpipe
joblib.dump(best_model, "custodial_suicide_rf_pipeline.pkl")

ğŸ“ˆ Evaluation

Confusion Matrix

Generated using a custom plotting function.

ROC Curve

Compared Logistic Regression vs Random Forest.

Cross-Validation

cv_scores = cross_val_score(logpipe, X, y, cv=5, scoring="f1")

ğŸ§¾ Project Structure

ğŸ“‚ CDO.ipynb                          â†’ ML workflow notebook  
ğŸ“‚ datasets/                           â†’ Crime in India dataset  
ğŸ“„ README.md                           â†’ Documentation  
ğŸ—‚ï¸ custodial_suicide_rf_pipeline.pkl  â†’ Trained Random Forest model

)

ğŸ”§ Technologies Used

Python â€¢ Pandas â€¢ NumPy â€¢ Scikit-Learn â€¢ Matplotlib â€¢ Seaborn â€¢ Joblib

ğŸš€ Results
	â€¢	Random Forest outperformed Logistic Regression.
	â€¢	Better performance across F1-score and ROCâ€“AUC.
	â€¢	Pipeline approach ensured clean preprocessing and stable predictions.
	â€¢	Final model exported for deployment.

ğŸ“Œ Future Improvements
	â€¢	Hyperparameter tuning (GridSearchCV/RandomizedSearchCV)
	â€¢	SMOTE for oversampling
	â€¢	Explainability via SHAP / LIME
	â€¢	Merging additional datasets for richer feature sets

ğŸ™Œ Acknowledgements

Dataset Source: Kaggle â€“ Crime in India (Salman Tokhi)
Tools: Python, Jupyter Notebook, Scikit-Learn
